{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Train.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdDYYRom75lu"
      },
      "source": [
        "# Importing Modules and Loading Data"
      ],
      "id": "hdDYYRom75lu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "788a95b3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from transformers import BertTokenizer, BertForMaskedLM"
      ],
      "id": "788a95b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx4JHnz36A7V"
      },
      "source": [
        "train_dataset = pd.read_csv('train.csv',escapechar=\"\\\\\",quoting=csv.QUOTE_NONE)"
      ],
      "id": "Qx4JHnz36A7V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr8y_g276Eac"
      },
      "source": [
        "test_dataset = pd.read_csv('test.csv',escapechar=\"\\\\\",quoting=csv.QUOTE_NONE)"
      ],
      "id": "hr8y_g276Eac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb1wlyqy7bQ9"
      },
      "source": [
        "# Data Exploration"
      ],
      "id": "mb1wlyqy7bQ9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgsJT4w66F1m"
      },
      "source": [
        "TRAIN_DATA_SIZE = len(train_dataset)\n",
        "TEST_DATA_SIZE = len(test_dataset)\n",
        "\n",
        "unique_train_bp = len(train_dataset[\"BULLET_POINTS\"].value_counts())\n",
        "unique_train_ds = len(train_dataset[\"DESCRIPTION\"].value_counts())\n",
        "unique_train_br = len(train_dataset[\"BRAND\"].value_counts())\n",
        "unique_train_tt = len(train_dataset[\"TITLE\"].value_counts())\n",
        "\n",
        "unique_test_bp = len(test_dataset[\"BULLET_POINTS\"].value_counts())\n",
        "unique_test_ds = len(test_dataset[\"DESCRIPTION\"].value_counts())\n",
        "unique_test_br = len(test_dataset[\"BRAND\"].value_counts())\n",
        "unique_test_tt = len(test_dataset[\"TITLE\"].value_counts())\n",
        "\n",
        "\n",
        "uniq_percent_bp = (unique_train_bp/TRAIN_DATA_SIZE, unique_test_bp/TEST_DATA_SIZE)\n",
        "uniq_percent_ds = (unique_train_ds/TRAIN_DATA_SIZE, unique_test_ds/TEST_DATA_SIZE)\n",
        "uniq_percent_br = (unique_train_br/TRAIN_DATA_SIZE, unique_test_br/TEST_DATA_SIZE)\n",
        "uniq_percent_tt = (unique_train_tt/TRAIN_DATA_SIZE, unique_test_tt/TEST_DATA_SIZE)\n",
        "\n",
        "print(\"To show uniqueness of data\\n\")\n",
        "\n",
        "print(\"Total Unique BULLET_POINTS               :\", (unique_train_bp, unique_test_bp))\n",
        "print(\"Total Unique DESCRIPTION                 :\", (unique_train_ds, unique_test_ds))\n",
        "print(\"Total Unique BRAND                       :\", (unique_train_br, unique_test_br))\n",
        "print(\"Total Unique TITLE                       :\", (unique_train_tt, unique_test_tt))\n",
        "\n",
        "print(\"Total Unique BULLET_POINTS in percentage :\", tuple([float(\"{0:.2f}\".format(n)) for n in uniq_percent_bp]))\n",
        "print(\"Total Unique DESCRIPTION in percentage   :\", tuple([float(\"{0:.2f}\".format(n)) for n in uniq_percent_ds]))\n",
        "print(\"Total Unique BRAND in percentage         :\", tuple([float(\"{0:.2f}\".format(n)) for n in uniq_percent_br]))\n",
        "print(\"Total Unique TITLE in percentage         :\", tuple([float(\"{0:.2f}\".format(n)) for n in uniq_percent_tt]))"
      ],
      "id": "fgsJT4w66F1m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUvcAOZ-6Iej"
      },
      "source": [
        "print(\"Total NaN values in train data's DESCRIPTION column :\",train_dataset[\"DESCRIPTION\"].isna().value_counts()[1])\n",
        "print(\"Total NaN values in train data's BULLET_POINTS column :\",train_dataset[\"BULLET_POINTS\"].isna().value_counts()[1])\n",
        "print(\"Total NaN values in train data's TITLE column :\",train_dataset[\"TITLE\"].isna().value_counts()[1])\n",
        "print(\"\\nTotal rows where both DESCRIPTION and BULLET_POINTS are missing (NaN):\", len(train_dataset[train_dataset[\"BULLET_POINTS\"].isna() & train_dataset[\"DESCRIPTION\"].isna()]))"
      ],
      "id": "iUvcAOZ-6Iej",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT8qE1_Y6Lxh"
      },
      "source": [
        "train_dataset.head(25)"
      ],
      "id": "bT8qE1_Y6Lxh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyT42kFx6L99"
      },
      "source": [
        "train_dataset.isna().sum()"
      ],
      "id": "QyT42kFx6L99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQgixP6x7f1u"
      },
      "source": [
        "# Data Cleaning"
      ],
      "id": "hQgixP6x7f1u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vx6Opfy6MKe"
      },
      "source": [
        "train_dataset.DESCRIPTION.fillna(train_dataset.TITLE, inplace=True)\n",
        "\n",
        "train_dataset = train_dataset[train_dataset[\"DESCRIPTION\"].notna()]\n",
        "\n",
        "train_dataset.TITLE.fillna(train_dataset.DESCRIPTION, inplace=True)\n",
        "\n",
        "train_dataset.BULLET_POINTS.fillna(train_dataset.DESCRIPTION, inplace=True)\n",
        "\n",
        "train_dataset=train_dataset.fillna('Unbranded')\n",
        "\n",
        "train_dataset = train_dataset.reset_index()"
      ],
      "id": "-vx6Opfy6MKe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYh2Il9W6MXB"
      },
      "source": [
        "df = train_dataset[['TITLE','DESCRIPTION','BULLET_POINTS','BRAND']]\n",
        "df"
      ],
      "id": "nYh2Il9W6MXB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrGOLZmm6Zrj"
      },
      "source": [
        "tok = WordPunctTokenizer()\n",
        "pattern1 = r'@[A-Za-z0-9]+'\n",
        "pattern2 = r'https?://[A-Za-z0-9./]+'\n",
        "pattern3 = r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n",
        "pattern4 = r'[\\|\\([{}\\])\\|]'\n",
        "pattern5 = r'[^\\w\\s]'\n",
        "\n",
        "pattern = r'|'.join((pattern1,pattern2))\n",
        "\n",
        "def clean_text(text):\n",
        "  #soup = BeautifulSoup(text, 'lxml')\n",
        "  notag = re.sub(pattern3, \" \", text)\n",
        "  #souped = soup.get_text()\n",
        "  clear = re.sub(pattern, '', notag)\n",
        "  clear2 = re.sub(pattern4, '', clear)\n",
        "  clear3 = re.sub(pattern5, ' ', clear2)\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "  stripped = emoji_pattern.sub(r'', clear3)\n",
        "  try:\n",
        "    clean = stripped.decode(\"utf-8-sig\".replace(u\"\\ufffd\",\"?\"))\n",
        "  except:\n",
        "    clean = stripped\n",
        "#   letters = re.sub(\"[^a-zA-Z]\",\" \", clean)\n",
        "  lower_case = clean.lower()\n",
        "#   words = tok.tokenize(lower_case)\n",
        "  return lower_case\n",
        "\n",
        "def clean_col(df,col, nums):\n",
        "    df = df[['TITLE','DESCRIPTION','BULLET_POINTS','BRAND','F2']]\n",
        "   # nums = [0,TRAIN_DATA_SIZE]\n",
        "    print(\"Cleaning and processing {}:\\n\".format(col))\n",
        "    cleaned_train_title = []\n",
        "    for i in range(nums[0],nums[1]):\n",
        "        if (( i+1 ) % 1 == 0):\n",
        "            cleaned_train_title.append(clean_text(df[col][i]))\n",
        "        if (( i+1 ) % 100000 == 0):\n",
        "            print(\"%d of %d have been processed\" % (i+1,nums[1]))\n",
        "    return cleaned_train_title"
      ],
      "id": "rrGOLZmm6Zrj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffMsk3DG7nG9"
      },
      "source": [
        "# Feature Engineering"
      ],
      "id": "ffMsk3DG7nG9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t6F22W36c2a"
      },
      "source": [
        "train_dataset['F1'] = train_dataset['BRAND'] + \" \" + train_dataset['TITLE']\n",
        "train_dataset['F2'] = train_dataset['TITLE'] + \" \" + train_dataset['BULLET_POINTS']"
      ],
      "id": "9t6F22W36c2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdp7bw2b6hbl"
      },
      "source": [
        "title_cleaned = clean_col(train_dataset,\"TITLE\", nums=[0,2903010])\n",
        "desc_cleaned = clean_col(train_dataset,\"DESCRIPTION\", nums=[0,2903010])\n",
        "bullet_cleaned = clean_col(train_dataset,\"BULLET_POINTS\", nums=[0,2903010])\n",
        "brand_cleaned = clean_col(train_dataset,\"BRAND\", nums=[0,2903010])\n",
        "f2_cleaned = clean_col(train_dataset,\"F2\", nums=[0,2903010])"
      ],
      "id": "jdp7bw2b6hbl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f34hQW4y6nrZ"
      },
      "source": [
        "cleaned_train_dataset = pd.DataFrame(\n",
        "    {\n",
        "     'TITLE': title_cleaned,\n",
        "     'DESCRIPTION': desc_cleaned,\n",
        "     'BULLET_POINTS': bullet_cleaned,\n",
        "     'BRAND': brand_cleaned,\n",
        "     'F2': f2_cleaned,\n",
        "     'BROWSE_NODE_ID' : train_dataset['BROWSE_NODE_ID']\n",
        "    })"
      ],
      "id": "f34hQW4y6nrZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT2FZsUx63vr"
      },
      "source": [
        "cleaned_train_data = cleaned_train_dataset.to_csv('cleaned_train_dataset.csv')"
      ],
      "id": "DT2FZsUx63vr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gylpIxJC7t3D"
      },
      "source": [
        "# Shuffling data for better sampling"
      ],
      "id": "gylpIxJC7t3D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1__GZ7g66tmB"
      },
      "source": [
        "def shuffler(df):\n",
        "  # return the pandas dataframe\n",
        "  return df.reindex(np.random.permutation(df.index))\n",
        "new_data = shuffler(cleaned_train_dataset)"
      ],
      "id": "1__GZ7g66tmB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeMJqfCc6v4W"
      },
      "source": [
        "df = cleaned_train_dataset.sample(frac=1).reset_index(drop=True)"
      ],
      "id": "VeMJqfCc6v4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09843989"
      },
      "source": [
        "# train_dataset = pd.read_csv('dataset/cleaned_train_dataset.csv')\n",
        "# test_dataset = pd.read_csv('dataset/cleaned_test_dataset.csv')\n",
        "# train_dataset.fillna(\"Null\", inplace=True)\n",
        "# test_dataset.fillna(\"Null\", inplace=True)\n",
        "# train_dataset = train_dataset[['TITLE','BULLET_POINTS','BROWSE_NODE_ID']]\n",
        "# test_dataset = test_dataset[['TITLE','BULLET_POINTS']]\n",
        "# train_dataset[\"CODE\"] = pd.Series(pd.factorize(train_dataset[\"BROWSE_NODE_ID\"])[0])"
      ],
      "id": "09843989",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a5fe288"
      },
      "source": [
        "COLUMN = \"TITLE\"\n",
        "TRAINER_SIZE = 100000\n",
        "SELECTED_VEC = \"CV\"\n",
        "#TFIDF_MAX_FEATURES = 500"
      ],
      "id": "7a5fe288",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-rh4B4l8Xoc"
      },
      "source": [
        "# Importing and Loading Embedding Files\n"
      ],
      "id": "X-rh4B4l8Xoc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuIWeKJc8Pow"
      },
      "source": [
        "GLOVE_EMBEDDING_FILE = 'E:\\dataset\\glove.840B.300d.txt\\glove.840B.300d.txt'\n",
        "FASTTEXT_EMBEDDING_FILE = 'E:\\dataset\\archive\\crawl-300d-2M.vec'\n",
        "WIKI_EMBEDDING_FILE =     '/kaggle/input/wikinews300d1mvec/wiki-news-300d-1M.vec'"
      ],
      "id": "zuIWeKJc8Pow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxClHgeK8Q_6"
      },
      "source": [
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "def load_embeddings(path):\n",
        "    with open(path,encoding=\"utf8\") as f:\n",
        "        return dict(get_coefs(*line.strip().split(' ')) for line in f)"
      ],
      "id": "xxClHgeK8Q_6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6YQgZug8T92"
      },
      "source": [
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    k = 0\n",
        "    i = 0\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "            a[word] = embeddings_index[word]\n",
        "            k += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            i += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "def build_vocab(sentences, verbose =  True):\n",
        "    \"\"\"\n",
        "    :param sentences: list of list of words\n",
        "    :return: dictionary of words and their count\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab\n",
        "\n",
        "def build_matrix(word_index, path):\n",
        "    embedding_index = load_embeddings(path)\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return embedding_matrix"
      ],
      "id": "B6YQgZug8T92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebmuWLUm7zw0"
      },
      "source": [
        "# Modelling data"
      ],
      "id": "ebmuWLUm7zw0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10b8a084"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "counts = train_dataset['CODE'].value_counts()\n",
        "new = train_dataset[train_dataset[\"CODE\"] == 677][:50000].append(train_dataset[train_dataset['CODE']!=677])\n",
        "new = new[new[\"CODE\"] == 5][:50000].append(new[new[\"CODE\"]!=5])\n",
        "new = new[new['CODE'].isin(counts[counts > 2].index)]\n",
        "\n",
        "new = new.sample(frac=1).reset_index(drop=True)\n",
        "X = new.drop([\"BROWSE_NODE_ID\",\"CODE\"], axis=1)[:TRAINER_SIZE]\n",
        "y = new[\"CODE\"][:TRAINER_SIZE]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "id": "10b8a084",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbac1ff1",
        "outputId": "b9a536da-c95d-4977-cdde-eba2d3c23d7e"
      },
      "source": [
        "lengthy = len(y.value_counts())\n",
        "lengthy"
      ],
      "id": "cbac1ff1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5736"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22d0588c"
      },
      "source": [
        "## KNN"
      ],
      "id": "22d0588c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "962a9661"
      },
      "source": [
        "if SELECTED_VEC == \"CV\":\n",
        "    print(\"Vectorizer : CV\")\n",
        "    cv = CountVectorizer()\n",
        "    X_trans_train = cv.fit_transform(X_train[COLUMN])\n",
        "    X_trans_test = cv.transform(X_test[COLUMN])\n",
        "    X_trans_test_real = cv.transform(test_dataset[COLUMN])\n",
        "if SELECTED_VEC == \"TFIDF\":\n",
        "    print(\"Vectorizer : TFIDF\")\n",
        "    td = TfidfVectorizer(max_features = TFIDF_MAX_FEATURES)\n",
        "    X_trans_train = td.fit_transform(X_train[COLUMN]).toarray()\n",
        "    X_trans_test=td.transform(X_test[COLUMN]).toarray()\n",
        "    X_trans_test_real = td.transform(test_dataset[COLUMN])"
      ],
      "id": "962a9661",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d2cd857"
      },
      "source": [
        "import pickle\n",
        "knn_models = []\n",
        "modelnames=[]\n",
        "for i in range(3,25):\n",
        "    modelname = \"knn_cv_n\"+str(i)\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_trans_train,y_train)\n",
        "    knn_models.append(knn)\n",
        "    modelnames.append(modelname)\n",
        "    with open(\"knn_models/\"+modelname+'.pkl', 'wb') as f:\n",
        "        pickle.dump(knn, f)\n",
        "    print(\"Wrote pkl file to knn_model/\"+modelname)"
      ],
      "id": "8d2cd857",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24c99224"
      },
      "source": [
        "def predict_on_real(i):\n",
        "    model = knn_models[i-3]\n",
        "    modelname = modelnames[i-3]\n",
        "    y_pred_real_1 = model.predict(X_trans_test_real[:50000])\n",
        "    y_pred_real_2 = model.predict(X_trans_test_real[50000:])\n",
        "    y1=pd.Series(y_pred_real_1)\n",
        "    y2=pd.Series(y_pred_real_2)\n",
        "    ans = y1.append(y2)\n",
        "    ans.index = range(1,len(test_dataset)+1)\n",
        "    ans.index.name = 'PRODUCT_ID'\n",
        "    ans.to_csv(\"answers/ans_\"+modelname+\".csv\", header=['BROWSE_NODE_ID'])\n",
        "    print(\"Wrote answers to \"+\"answers/ans_\"+modelname+\".csv\")\n",
        "    return ans"
      ],
      "id": "24c99224",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48cd9628"
      },
      "source": [
        "for i in range(3,25,2):\n",
        "    predict_on_real(i)"
      ],
      "id": "48cd9628",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46f6ea93"
      },
      "source": [
        "# Deep Learning"
      ],
      "id": "46f6ea93"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9796c17e"
      },
      "source": [
        "from keras.layers import add, concatenate, Conv1D, MaxPooling1D, merge\n",
        "from keras.layers import Embedding \n",
        "# from keras.utils import plot_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "# from keras.utils import to_categorical"
      ],
      "id": "9796c17e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f915a8af"
      },
      "source": [
        ""
      ],
      "id": "f915a8af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3efc7667"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(X_train.TITLE.values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "X_trans_train = tokenizer.texts_to_sequences(X_train.TITLE.values)\n",
        "X_trans_train = pad_sequences(X_trans_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X_trans_train.shape)\n",
        "\n",
        "X_trans_test = tokenizer.texts_to_sequences(X_test.TITLE.values)\n",
        "X_trans_test = pad_sequences(X_trans_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X_trans_test.shape)\n",
        "\n",
        "X_trans_test_real = tokenizer.texts_to_sequences(test_dataset.TITLE.values)\n",
        "X_trans_test_real = pad_sequences(X_trans_test_real, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X_trans_train.shape)\n",
        "\n",
        "\n",
        "Y = y\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "id": "3efc7667",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60453c55"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_shape=(X_trans_train.shape[1],)))\n",
        "model.add(Dense(400, activation = 'relu'))\n",
        "model.add(Dense(300, activation = 'relu'))\n",
        "model.add(Dense(9919, activation = 'softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_loss',patience=5, min_delta=0.0001)\n",
        "model_checkpoint = ModelCheckpoint(filepath='./model-weights.hdf5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "callbacks = [\n",
        "    earlystop, \n",
        "    model_checkpoint\n",
        "]\n",
        "\n"
      ],
      "id": "60453c55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "049e04ea"
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 1024\n",
        "\n",
        "history = model.fit(X_trans_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=callbacks)"
      ],
      "id": "049e04ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7298f11"
      },
      "source": [
        "# Transformers"
      ],
      "id": "c7298f11"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4849498e"
      },
      "source": [
        "seq_len = 512\n",
        "num_samples = len(X)\n",
        "\n",
        "# initialize empty zero arrays\n",
        "Xids = np.zeros((num_samples, seq_len))\n",
        "Xmask = np.zeros((num_samples, seq_len))\n",
        "\n",
        "# check shape\n",
        "Xids.shape\n",
        "\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "for i, phrase in enumerate(X['TITLE']):\n",
        "    tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True,\n",
        "                                   padding='max_length', add_special_tokens=True,\n",
        "                                   return_tensors='tf')\n",
        "    # assign tokenized outputs to respective rows in numpy arrays\n",
        "    Xids[i, :] = tokens['input_ids']\n",
        "    Xmask[i, :] = tokens['attention_mask']"
      ],
      "id": "4849498e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "316834df"
      },
      "source": [
        "labels=y\n",
        "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
        "\n",
        "def map_func(input_ids, masks, labels):\n",
        "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
        "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
        "\n",
        "# then we use the dataset map method to apply this transformation\n",
        "dataset = dataset.map(map_func)"
      ],
      "id": "316834df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d1c9aca"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "# shuffle and batch - dropping any remaining samples that don't cleanly\n",
        "# fit into a batch of 16\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ],
      "id": "7d1c9aca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6969e12"
      },
      "source": [
        "split = 0.9\n",
        "size = int((Xids.shape[0]/batch_size)*split)\n",
        "\n",
        "# get training and validation sets\n",
        "train_ds = dataset.take(size)\n",
        "val_ds = dataset.skip(size)"
      ],
      "id": "c6969e12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6690d73",
        "outputId": "22f25c0b-73ab-43e8-fe93-8485719b6008"
      },
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "bert = TFAutoModel.from_pretrained('bert-base-cased')"
      ],
      "id": "f6690d73",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d205b1b"
      },
      "source": [
        "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
        "\n",
        "# we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
        "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access pooled activations with [1]\n",
        "\n",
        "# convert bert embeddings into 5 output classes\n",
        "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
        "y = tf.keras.layers.Dense(lengthy, activation='softmax', name='outputs')(x)"
      ],
      "id": "6d205b1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2dc9853"
      },
      "source": [
        "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "acc = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
      ],
      "id": "e2dc9853",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad999273",
        "outputId": "ebad071a-8aa6-422a-fc87-78e414a9de6e"
      },
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=1,\n",
        "    verbose=1\n",
        ")"
      ],
      "id": "ad999273",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   2/5625 [..............................] - ETA: 90:16:37 - loss: 8.7278 - accuracy: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4031313"
      },
      "source": [
        ""
      ],
      "id": "d4031313",
      "execution_count": null,
      "outputs": []
    }
  ]
}